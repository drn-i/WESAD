{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IH1tYCgeKhqC",
        "outputId": "a1a7f9ff-5195-417d-edee-324401eeaf40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Hypertension-AI/requirements.txt (line 1)) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.44.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Hypertension-AI/requirements.txt (line 2)) (4.57.0)\n",
            "Requirement already satisfied: datasets>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Hypertension-AI/requirements.txt (line 3)) (4.0.0)\n",
            "Requirement already satisfied: peft>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Hypertension-AI/requirements.txt (line 4)) (0.17.1)\n",
            "Requirement already satisfied: accelerate>=0.33.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Hypertension-AI/requirements.txt (line 5)) (1.10.1)\n",
            "Collecting bitsandbytes>=0.43.0 (from -r /content/Hypertension-AI/requirements.txt (line 6))\n",
            "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Hypertension-AI/requirements.txt (line 7)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Hypertension-AI/requirements.txt (line 8)) (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Hypertension-AI/requirements.txt (line 9)) (2.0.2)\n",
            "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Hypertension-AI/requirements.txt (line 10)) (1.5.2)\n",
            "Requirement already satisfied: tqdm>=4.66.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Hypertension-AI/requirements.txt (line 11)) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.0->-r /content/Hypertension-AI/requirements.txt (line 2)) (0.35.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.0->-r /content/Hypertension-AI/requirements.txt (line 2)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.0->-r /content/Hypertension-AI/requirements.txt (line 2)) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.0->-r /content/Hypertension-AI/requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.0->-r /content/Hypertension-AI/requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.0->-r /content/Hypertension-AI/requirements.txt (line 2)) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.0->-r /content/Hypertension-AI/requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.20.0->-r /content/Hypertension-AI/requirements.txt (line 3)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.20.0->-r /content/Hypertension-AI/requirements.txt (line 3)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.20.0->-r /content/Hypertension-AI/requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.20.0->-r /content/Hypertension-AI/requirements.txt (line 3)) (0.70.16)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft>=0.11.0->-r /content/Hypertension-AI/requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 7)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.0->-r /content/Hypertension-AI/requirements.txt (line 8)) (1.16.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.0->-r /content/Hypertension-AI/requirements.txt (line 8)) (3.6.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20.0->-r /content/Hypertension-AI/requirements.txt (line 3)) (3.13.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.44.0->-r /content/Hypertension-AI/requirements.txt (line 2)) (1.1.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 7)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.44.0->-r /content/Hypertension-AI/requirements.txt (line 2)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.44.0->-r /content/Hypertension-AI/requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.44.0->-r /content/Hypertension-AI/requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.44.0->-r /content/Hypertension-AI/requirements.txt (line 2)) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.2.0->-r /content/Hypertension-AI/requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20.0->-r /content/Hypertension-AI/requirements.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20.0->-r /content/Hypertension-AI/requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20.0->-r /content/Hypertension-AI/requirements.txt (line 3)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20.0->-r /content/Hypertension-AI/requirements.txt (line 3)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20.0->-r /content/Hypertension-AI/requirements.txt (line 3)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20.0->-r /content/Hypertension-AI/requirements.txt (line 3)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20.0->-r /content/Hypertension-AI/requirements.txt (line 3)) (1.22.0)\n",
            "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n",
        "\n",
        "!pip install -r /content/Hypertension-AI/requirements.txt\n",
        "\n",
        "#!pip install -q -U transformers datasets accelerate peft trl bitsandbytes scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4beCcV84hLgS",
        "outputId": "584b05be-1cb0-4266-b7d6-375ba22cdc28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"train\": 1270,\n",
            "  \"valid\": 318,\n",
            "  \"test\": 397,\n",
            "  \"stress_thresholds\": {\n",
            "    \"low_max\": 6.0,\n",
            "    \"moderate_max\": 10.0\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!python /content/Hypertension-AI/src/prepare_hypertension_llm_data.py \\\n",
        "  --data-path /content/Hypertension-AI/data/hypertension_dataset.csv \\\n",
        "  --output-dir /content/Hypertension-AI/data/llm \\\n",
        "  --test-size 0.2 \\\n",
        "  --valid-size 0.2 \\\n",
        "  --seed 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z2hZicU0hL_K",
        "outputId": "9f3372c6-cff4-4d3f-9003-f2de7e73ccec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-08 06:34:28.244378: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1759905268.264130     772 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1759905268.270169     772 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1759905268.285640     772 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759905268.285663     772 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759905268.285667     772 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759905268.285673     772 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-08 06:34:28.290828: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "tokenizer_config.json: 100% 54.5k/54.5k [00:00<00:00, 114MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 19.8MB/s]\n",
            "special_tokens_map.json: 100% 296/296 [00:00<00:00, 2.88MB/s]\n",
            "Generating train split: 1270 examples [00:00, 99892.47 examples/s]\n",
            "Generating validation split: 318 examples [00:00, 200118.33 examples/s]\n",
            "config.json: 100% 877/877 [00:00<00:00, 5.91MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors: 100% 2.47G/2.47G [00:48<00:00, 51.2MB/s]\n",
            "generation_config.json: 100% 189/189 [00:00<00:00, 1.55MB/s]\n",
            "trainable params: 1,703,936 || all params: 1,237,518,336 || trainable%: 0.1377\n",
            "Map: 100% 1270/1270 [00:00<00:00, 1965.66 examples/s]\n",
            "Map: 100% 318/318 [00:00<00:00, 1984.94 examples/s]\n",
            "  0% 0/240 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "{'loss': 2.1228, 'grad_norm': 5.264904022216797, 'learning_rate': 7.6e-05, 'epoch': 0.25}\n",
            "{'loss': 0.2766, 'grad_norm': 3.005455493927002, 'learning_rate': 0.00015600000000000002, 'epoch': 0.5}\n",
            "{'loss': 0.0704, 'grad_norm': 1.3446465730667114, 'learning_rate': 0.0001905263157894737, 'epoch': 0.76}\n",
            "{'loss': 0.0408, 'grad_norm': 0.6674843430519104, 'learning_rate': 0.00016947368421052633, 'epoch': 1.0}\n",
            "{'loss': 0.0399, 'grad_norm': 0.6748101711273193, 'learning_rate': 0.00014842105263157895, 'epoch': 1.25}\n",
            " 42% 100/240 [02:54<04:12,  1.80s/it]\n",
            "  0% 0/159 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 3/159 [00:00<00:08, 18.73it/s]\u001b[A\n",
            "  3% 5/159 [00:00<00:09, 15.67it/s]\u001b[A\n",
            "  4% 7/159 [00:00<00:10, 14.61it/s]\u001b[A\n",
            "  6% 9/159 [00:00<00:10, 14.12it/s]\u001b[A\n",
            "  7% 11/159 [00:00<00:10, 13.75it/s]\u001b[A\n",
            "  8% 13/159 [00:00<00:10, 13.43it/s]\u001b[A\n",
            "  9% 15/159 [00:01<00:10, 13.28it/s]\u001b[A\n",
            " 11% 17/159 [00:01<00:10, 13.05it/s]\u001b[A\n",
            " 12% 19/159 [00:01<00:10, 13.13it/s]\u001b[A\n",
            " 13% 21/159 [00:01<00:10, 13.21it/s]\u001b[A\n",
            " 14% 23/159 [00:01<00:10, 13.09it/s]\u001b[A\n",
            " 16% 25/159 [00:01<00:10, 13.15it/s]\u001b[A\n",
            " 17% 27/159 [00:01<00:09, 13.20it/s]\u001b[A\n",
            " 18% 29/159 [00:02<00:10, 12.92it/s]\u001b[A\n",
            " 19% 31/159 [00:02<00:09, 13.08it/s]\u001b[A\n",
            " 21% 33/159 [00:02<00:09, 13.19it/s]\u001b[A\n",
            " 22% 35/159 [00:02<00:09, 13.23it/s]\u001b[A\n",
            " 23% 37/159 [00:02<00:09, 13.24it/s]\u001b[A\n",
            " 25% 39/159 [00:02<00:09, 13.19it/s]\u001b[A\n",
            " 26% 41/159 [00:03<00:08, 13.14it/s]\u001b[A\n",
            " 27% 43/159 [00:03<00:09, 12.81it/s]\u001b[A\n",
            " 28% 45/159 [00:03<00:08, 12.98it/s]\u001b[A\n",
            " 30% 47/159 [00:03<00:08, 13.04it/s]\u001b[A\n",
            " 31% 49/159 [00:03<00:08, 13.11it/s]\u001b[A\n",
            " 32% 51/159 [00:03<00:08, 13.03it/s]\u001b[A\n",
            " 33% 53/159 [00:03<00:08, 13.10it/s]\u001b[A\n",
            " 35% 55/159 [00:04<00:07, 13.08it/s]\u001b[A\n",
            " 36% 57/159 [00:04<00:07, 12.95it/s]\u001b[A\n",
            " 37% 59/159 [00:04<00:07, 13.08it/s]\u001b[A\n",
            " 38% 61/159 [00:04<00:07, 13.20it/s]\u001b[A\n",
            " 40% 63/159 [00:04<00:07, 13.07it/s]\u001b[A\n",
            " 41% 65/159 [00:04<00:07, 13.15it/s]\u001b[A\n",
            " 42% 67/159 [00:05<00:07, 13.11it/s]\u001b[A\n",
            " 43% 69/159 [00:05<00:06, 13.21it/s]\u001b[A\n",
            " 45% 71/159 [00:05<00:06, 12.88it/s]\u001b[A\n",
            " 46% 73/159 [00:05<00:06, 13.05it/s]\u001b[A\n",
            " 47% 75/159 [00:05<00:06, 12.97it/s]\u001b[A\n",
            " 48% 77/159 [00:05<00:06, 12.99it/s]\u001b[A\n",
            " 50% 79/159 [00:05<00:06, 13.02it/s]\u001b[A\n",
            " 51% 81/159 [00:06<00:05, 13.11it/s]\u001b[A\n",
            " 52% 83/159 [00:06<00:05, 12.83it/s]\u001b[A\n",
            " 53% 85/159 [00:06<00:05, 13.01it/s]\u001b[A\n",
            " 55% 87/159 [00:06<00:05, 13.04it/s]\u001b[A\n",
            " 56% 89/159 [00:06<00:05, 13.10it/s]\u001b[A\n",
            " 57% 91/159 [00:06<00:05, 12.97it/s]\u001b[A\n",
            " 58% 93/159 [00:07<00:05, 12.96it/s]\u001b[A\n",
            " 60% 95/159 [00:07<00:04, 13.08it/s]\u001b[A\n",
            " 61% 97/159 [00:07<00:04, 12.83it/s]\u001b[A\n",
            " 62% 99/159 [00:07<00:04, 12.95it/s]\u001b[A\n",
            " 64% 101/159 [00:07<00:04, 13.09it/s]\u001b[A\n",
            " 65% 103/159 [00:07<00:04, 12.99it/s]\u001b[A\n",
            " 66% 105/159 [00:07<00:04, 13.05it/s]\u001b[A\n",
            " 67% 107/159 [00:08<00:03, 13.07it/s]\u001b[A\n",
            " 69% 109/159 [00:08<00:03, 13.19it/s]\u001b[A\n",
            " 70% 111/159 [00:08<00:03, 12.80it/s]\u001b[A\n",
            " 71% 113/159 [00:08<00:03, 12.96it/s]\u001b[A\n",
            " 72% 115/159 [00:08<00:03, 12.88it/s]\u001b[A\n",
            " 74% 117/159 [00:08<00:03, 12.98it/s]\u001b[A\n",
            " 75% 119/159 [00:09<00:03, 12.49it/s]\u001b[A\n",
            " 76% 121/159 [00:09<00:03, 11.65it/s]\u001b[A\n",
            " 77% 123/159 [00:09<00:03, 11.07it/s]\u001b[A\n",
            " 79% 125/159 [00:09<00:03, 11.10it/s]\u001b[A\n",
            " 80% 127/159 [00:09<00:02, 10.75it/s]\u001b[A\n",
            " 81% 129/159 [00:10<00:02, 10.86it/s]\u001b[A\n",
            " 82% 131/159 [00:10<00:02, 11.04it/s]\u001b[A\n",
            " 84% 133/159 [00:10<00:02, 10.93it/s]\u001b[A\n",
            " 85% 135/159 [00:10<00:02, 10.28it/s]\u001b[A\n",
            " 86% 137/159 [00:10<00:02, 10.12it/s]\u001b[A\n",
            " 87% 139/159 [00:11<00:02,  9.93it/s]\u001b[A\n",
            " 88% 140/159 [00:11<00:01,  9.81it/s]\u001b[A\n",
            " 89% 142/159 [00:11<00:01, 10.46it/s]\u001b[A\n",
            " 91% 144/159 [00:11<00:01, 11.28it/s]\u001b[A\n",
            " 92% 146/159 [00:11<00:01, 11.46it/s]\u001b[A\n",
            " 93% 148/159 [00:11<00:00, 11.90it/s]\u001b[A\n",
            " 94% 150/159 [00:11<00:00, 12.28it/s]\u001b[A\n",
            " 96% 152/159 [00:12<00:00, 12.49it/s]\u001b[A\n",
            " 97% 154/159 [00:12<00:00, 12.75it/s]\u001b[A\n",
            " 98% 156/159 [00:12<00:00, 12.93it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.03582773730158806, 'eval_runtime': 12.6862, 'eval_samples_per_second': 25.067, 'eval_steps_per_second': 12.533, 'epoch': 1.25}\n",
            " 42% 100/240 [03:06<04:12,  1.80s/it]\n",
            "100% 159/159 [00:12<00:00, 13.01it/s]\u001b[A\n",
            "{'loss': 0.0317, 'grad_norm': 1.246176838874817, 'learning_rate': 0.00012736842105263158, 'epoch': 1.5}\n",
            "{'loss': 0.0314, 'grad_norm': 0.537592351436615, 'learning_rate': 0.00010631578947368421, 'epoch': 1.76}\n",
            "{'loss': 0.0269, 'grad_norm': 1.2977625131607056, 'learning_rate': 8.526315789473685e-05, 'epoch': 2.0}\n",
            "{'loss': 0.0213, 'grad_norm': 0.4682394862174988, 'learning_rate': 6.421052631578948e-05, 'epoch': 2.25}\n",
            "{'loss': 0.021, 'grad_norm': 1.3189014196395874, 'learning_rate': 4.3157894736842105e-05, 'epoch': 2.5}\n",
            " 83% 200/240 [05:59<01:09,  1.73s/it]\n",
            "  0% 0/159 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 3/159 [00:00<00:08, 19.23it/s]\u001b[A\n",
            "  3% 5/159 [00:00<00:09, 15.66it/s]\u001b[A\n",
            "  4% 7/159 [00:00<00:10, 14.43it/s]\u001b[A\n",
            "  6% 9/159 [00:00<00:10, 13.95it/s]\u001b[A\n",
            "  7% 11/159 [00:00<00:11, 13.24it/s]\u001b[A\n",
            "  8% 13/159 [00:00<00:11, 13.15it/s]\u001b[A\n",
            "  9% 15/159 [00:01<00:11, 13.00it/s]\u001b[A\n",
            " 11% 17/159 [00:01<00:10, 12.98it/s]\u001b[A\n",
            " 12% 19/159 [00:01<00:10, 12.94it/s]\u001b[A\n",
            " 13% 21/159 [00:01<00:10, 12.82it/s]\u001b[A\n",
            " 14% 23/159 [00:01<00:10, 12.51it/s]\u001b[A\n",
            " 16% 25/159 [00:01<00:10, 12.67it/s]\u001b[A\n",
            " 17% 27/159 [00:02<00:10, 12.65it/s]\u001b[A\n",
            " 18% 29/159 [00:02<00:10, 12.81it/s]\u001b[A\n",
            " 19% 31/159 [00:02<00:10, 12.78it/s]\u001b[A\n",
            " 21% 33/159 [00:02<00:09, 12.78it/s]\u001b[A\n",
            " 22% 35/159 [00:02<00:09, 12.89it/s]\u001b[A\n",
            " 23% 37/159 [00:02<00:09, 12.75it/s]\u001b[A\n",
            " 25% 39/159 [00:02<00:09, 12.75it/s]\u001b[A\n",
            " 26% 41/159 [00:03<00:09, 12.88it/s]\u001b[A\n",
            " 27% 43/159 [00:03<00:08, 13.00it/s]\u001b[A\n",
            " 28% 45/159 [00:03<00:08, 13.14it/s]\u001b[A\n",
            " 30% 47/159 [00:03<00:08, 13.14it/s]\u001b[A\n",
            " 31% 49/159 [00:03<00:08, 13.24it/s]\u001b[A\n",
            " 32% 51/159 [00:03<00:08, 12.81it/s]\u001b[A\n",
            " 33% 53/159 [00:04<00:08, 12.92it/s]\u001b[A\n",
            " 35% 55/159 [00:04<00:07, 13.06it/s]\u001b[A\n",
            " 36% 57/159 [00:04<00:07, 13.12it/s]\u001b[A\n",
            " 37% 59/159 [00:04<00:07, 13.25it/s]\u001b[A\n",
            " 38% 61/159 [00:04<00:07, 13.24it/s]\u001b[A\n",
            " 40% 63/159 [00:04<00:07, 13.05it/s]\u001b[A\n",
            " 41% 65/159 [00:04<00:07, 12.93it/s]\u001b[A\n",
            " 42% 67/159 [00:05<00:07, 12.97it/s]\u001b[A\n",
            " 43% 69/159 [00:05<00:06, 13.06it/s]\u001b[A\n",
            " 45% 71/159 [00:05<00:06, 13.06it/s]\u001b[A\n",
            " 46% 73/159 [00:05<00:06, 13.14it/s]\u001b[A\n",
            " 47% 75/159 [00:05<00:07, 11.81it/s]\u001b[A\n",
            " 48% 77/159 [00:05<00:07, 11.11it/s]\u001b[A\n",
            " 50% 79/159 [00:06<00:07, 10.91it/s]\u001b[A\n",
            " 51% 81/159 [00:06<00:07, 10.94it/s]\u001b[A\n",
            " 52% 83/159 [00:06<00:06, 10.86it/s]\u001b[A\n",
            " 53% 85/159 [00:06<00:06, 10.90it/s]\u001b[A\n",
            " 55% 87/159 [00:06<00:06, 10.67it/s]\u001b[A\n",
            " 56% 89/159 [00:07<00:06, 10.06it/s]\u001b[A\n",
            " 57% 91/159 [00:07<00:06, 10.04it/s]\u001b[A\n",
            " 58% 93/159 [00:07<00:06, 10.00it/s]\u001b[A\n",
            " 60% 95/159 [00:07<00:06,  9.89it/s]\u001b[A\n",
            " 61% 97/159 [00:07<00:05, 10.68it/s]\u001b[A\n",
            " 62% 99/159 [00:08<00:05, 10.98it/s]\u001b[A\n",
            " 64% 101/159 [00:08<00:05, 11.58it/s]\u001b[A\n",
            " 65% 103/159 [00:08<00:04, 12.07it/s]\u001b[A\n",
            " 66% 105/159 [00:08<00:04, 12.38it/s]\u001b[A\n",
            " 67% 107/159 [00:08<00:04, 12.61it/s]\u001b[A\n",
            " 69% 109/159 [00:08<00:03, 12.84it/s]\u001b[A\n",
            " 70% 111/159 [00:09<00:03, 12.53it/s]\u001b[A\n",
            " 71% 113/159 [00:09<00:03, 12.78it/s]\u001b[A\n",
            " 72% 115/159 [00:09<00:03, 12.82it/s]\u001b[A\n",
            " 74% 117/159 [00:09<00:03, 12.98it/s]\u001b[A\n",
            " 75% 119/159 [00:09<00:03, 12.99it/s]\u001b[A\n",
            " 76% 121/159 [00:09<00:02, 13.10it/s]\u001b[A\n",
            " 77% 123/159 [00:09<00:02, 12.98it/s]\u001b[A\n",
            " 79% 125/159 [00:10<00:02, 12.48it/s]\u001b[A\n",
            " 80% 127/159 [00:10<00:02, 12.28it/s]\u001b[A\n",
            " 81% 129/159 [00:10<00:02, 12.50it/s]\u001b[A\n",
            " 82% 131/159 [00:10<00:02, 12.61it/s]\u001b[A\n",
            " 84% 133/159 [00:10<00:02, 12.87it/s]\u001b[A\n",
            " 85% 135/159 [00:10<00:01, 12.86it/s]\u001b[A\n",
            " 86% 137/159 [00:11<00:01, 12.97it/s]\u001b[A\n",
            " 87% 139/159 [00:11<00:01, 12.71it/s]\u001b[A\n",
            " 89% 141/159 [00:11<00:01, 12.90it/s]\u001b[A\n",
            " 90% 143/159 [00:11<00:01, 13.02it/s]\u001b[A\n",
            " 91% 145/159 [00:11<00:01, 13.08it/s]\u001b[A\n",
            " 92% 147/159 [00:11<00:00, 13.14it/s]\u001b[A\n",
            " 94% 149/159 [00:11<00:00, 13.16it/s]\u001b[A\n",
            " 95% 151/159 [00:12<00:00, 12.68it/s]\u001b[A\n",
            " 96% 153/159 [00:12<00:00, 12.65it/s]\u001b[A\n",
            " 97% 155/159 [00:12<00:00, 12.75it/s]\u001b[A\n",
            " 99% 157/159 [00:12<00:00, 12.71it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.018925828859210014, 'eval_runtime': 12.8178, 'eval_samples_per_second': 24.809, 'eval_steps_per_second': 12.405, 'epoch': 2.5}\n",
            " 83% 200/240 [06:12<01:09,  1.73s/it]\n",
            "100% 159/159 [00:12<00:00, 12.77it/s]\u001b[A\n",
            "{'loss': 0.0202, 'grad_norm': 0.36945509910583496, 'learning_rate': 2.2105263157894736e-05, 'epoch': 2.76}\n",
            "{'loss': 0.0168, 'grad_norm': 0.20522890985012054, 'learning_rate': 1.0526315789473685e-06, 'epoch': 3.0}\n",
            "{'train_runtime': 441.349, 'train_samples_per_second': 8.633, 'train_steps_per_second': 0.544, 'train_loss': 0.2266542311757803, 'epoch': 3.0}\n",
            "100% 240/240 [07:21<00:00,  1.84s/it]\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Caching is incompatible with gradient checkpointing in LlamaDecoderLayer. Setting `past_key_values=None`.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "{\n",
            "  \"adapter_dir\": \"/content/Hypertension-AI/artifacts/llm/lora_adapter\",\n",
            "  \"validation_metrics\": {\n",
            "    \"num_samples\": 318,\n",
            "    \"hypertension_accuracy\": 0.0,\n",
            "    \"stress_accuracy\": 0.0,\n",
            "    \"joint_exact_match\": 0.0\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!python /content/Hypertension-AI/src/fine_tune_hypertension_llm.py \\\n",
        "  --base-model meta-llama/Llama-3.2-1B-Instruct \\\n",
        "  --train-file /content/Hypertension-AI/data/llm/train.jsonl \\\n",
        "  --valid-file /content/Hypertension-AI/data/llm/valid.jsonl \\\n",
        "  --output-dir /content/Hypertension-AI/artifacts/llm \\\n",
        "  --num-epochs 3 \\\n",
        "  --batch-size 2 \\\n",
        "  --gradient-accumulation 8 \\\n",
        "  --learning-rate 2e-4 \\\n",
        "  --max-length 512 \\\n",
        "  --max-new-tokens 16 \\\n",
        "  --eval-steps 100 \\\n",
        "  --logging-steps 20 \\\n",
        "  --save-steps 500 \\\n",
        "  --lora-r 16 \\\n",
        "  --lora-alpha 32 \\\n",
        "  --lora-dropout 0.05 \\\n",
        "  --load-in-4bit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOFkftPdPm_B"
      },
      "source": [
        "{\n",
        "  \"adapter_dir\": \"/content/Hypertension-AI/artifacts/llm/lora_adapter\",\n",
        "  \"validation_metrics\": {\n",
        "    \"num_samples\": 318,\n",
        "    \"hypertension_accuracy\": 0.0,\n",
        "    \"stress_accuracy\": 0.0,\n",
        "    \"joint_exact_match\": 0.0\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7lMEfmNvbzNE",
        "outputId": "b7531c07-5b48-4c65-f6bd-9ec952190229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-08 06:47:56.721690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1759906076.755947    4247 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1759906076.766458    4247 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1759906076.791405    4247 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759906076.791435    4247 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759906076.791443    4247 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759906076.791449    4247 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-08 06:47:56.798611: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Generating test split: 397 examples [00:00, 105228.68 examples/s]\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "{\n",
            "  \"adapter_dir\": \"/content/Hypertension-AI/artifacts/llm/lora_adapter\",\n",
            "  \"test_metrics\": {\n",
            "    \"num_samples\": 397,\n",
            "    \"hypertension_accuracy\": 0.7304785894206549,\n",
            "    \"stress_accuracy\": 0.8765743073047859,\n",
            "    \"joint_exact_match\": 0.6700251889168766\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!python /content/Hypertension-AI/src/fine_tune_hypertension_llm.py \\\n",
        "  --eval-only \\\n",
        "  --base-model meta-llama/Llama-3.2-1B-Instruct \\\n",
        "  --adapter-dir /content/Hypertension-AI/artifacts/llm/lora_adapter \\\n",
        "  --test-file /content/Hypertension-AI/data/llm/test.jsonl \\\n",
        "  --max-new-tokens 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aiE-R8DbeXjh",
        "outputId": "2162fdb1-36fd-44ed-be34-32e9eff0eeb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-08 06:50:47.146170: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1759906247.176412    5015 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1759906247.182284    5015 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1759906247.197724    5015 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759906247.197750    5015 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759906247.197755    5015 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759906247.197760    5015 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-08 06:50:47.202307: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Generating data split: 397 examples [00:00, 36559.49 examples/s]\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "{\n",
            "  \"n_samples\": 397,\n",
            "  \"hypertension\": {\n",
            "    \"tp\": 206,\n",
            "    \"tn\": 84,\n",
            "    \"fp\": 107,\n",
            "    \"fn\": 0,\n",
            "    \"accuracy\": 0.7304785894206549,\n",
            "    \"precision\": 0.65814696485623,\n",
            "    \"recall\": 1.0,\n",
            "    \"specificity\": 0.4397905759162304,\n",
            "    \"f1\": 0.7938342967244701\n",
            "  },\n",
            "  \"stress\": {\n",
            "    \"accuracy\": 0.8765743073047859\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!python /content/Hypertension-AI/src/evaluate_llm_detailed.py \\\n",
        "  --base-model meta-llama/Llama-3.2-1B-Instruct \\\n",
        "  --adapter-dir /content/Hypertension-AI/artifacts/llm/lora_adapter \\\n",
        "  --data-file /content/Hypertension-AI/data/llm/test.jsonl \\\n",
        "  --max-new-tokens 24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD1RHiXsSBKW"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Using the second version of Scripts**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J2tTFnjNhHkt",
        "outputId": "c201c185-a0a5-4cee-fdfc-bcce095de568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"train\": 1607,\n",
            "  \"valid\": 179,\n",
            "  \"test\": 199,\n",
            "  \"stress_thresholds\": {\n",
            "    \"low_max\": 6.0,\n",
            "    \"moderate_max\": 10.0\n",
            "  },\n",
            "  \"prompt_style\": \"chat_template_llama\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!python /content/Hypertension-AI/src/prepare_hypertension_llm_data_2.py \\\n",
        "  --data-path /content/Hypertension-AI/data/hypertension_dataset.csv \\\n",
        "  --output-dir /content/Hypertension-AI/data/llm \\\n",
        "  --base-model meta-llama/Llama-3.2-1B-Instruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wAJGAkfwhIiY",
        "outputId": "762948c1-7200-48b7-ddbe-6a8ff8f2853e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-08 06:53:45.663254: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1759906425.708301    5846 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1759906425.718643    5846 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1759906425.743412    5846 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759906425.743450    5846 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759906425.743457    5846 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759906425.743464    5846 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-08 06:53:45.755111: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Generating train split: 1607 examples [00:00, 199038.70 examples/s]\n",
            "Generating validation split: 179 examples [00:00, 110279.14 examples/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n",
            "Map: 100% 1607/1607 [00:01<00:00, 1042.87 examples/s]\n",
            "Map: 100% 179/179 [00:00<00:00, 1424.20 examples/s]\n",
            "  0% 0/303 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "{'loss': 0.1478, 'grad_norm': 2.071502923965454, 'learning_rate': 3.8e-05, 'epoch': 0.2}\n",
            "{'loss': 0.0674, 'grad_norm': 0.576038122177124, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.4}\n",
            "{'loss': 0.0514, 'grad_norm': 1.3780238628387451, 'learning_rate': 9.644268774703557e-05, 'epoch': 0.6}\n",
            "{'loss': 0.0379, 'grad_norm': 0.44954970479011536, 'learning_rate': 8.853754940711463e-05, 'epoch': 0.8}\n",
            "{'loss': 0.0358, 'grad_norm': 0.18819451332092285, 'learning_rate': 8.063241106719368e-05, 'epoch': 1.0}\n",
            " 33% 100/303 [04:31<09:07,  2.70s/it]\n",
            "  0% 0/90 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/90 [00:00<00:04, 19.98it/s]\u001b[A\n",
            "  4% 4/90 [00:00<00:07, 11.66it/s]\u001b[A\n",
            "  7% 6/90 [00:00<00:07, 10.52it/s]\u001b[A\n",
            "  9% 8/90 [00:00<00:08,  9.88it/s]\u001b[A\n",
            " 11% 10/90 [00:00<00:08,  9.43it/s]\u001b[A\n",
            " 12% 11/90 [00:01<00:08,  9.46it/s]\u001b[A\n",
            " 13% 12/90 [00:01<00:08,  9.46it/s]\u001b[A\n",
            " 14% 13/90 [00:01<00:08,  9.48it/s]\u001b[A\n",
            " 16% 14/90 [00:01<00:08,  9.36it/s]\u001b[A\n",
            " 17% 15/90 [00:01<00:07,  9.46it/s]\u001b[A\n",
            " 18% 16/90 [00:01<00:07,  9.41it/s]\u001b[A\n",
            " 19% 17/90 [00:01<00:07,  9.45it/s]\u001b[A\n",
            " 20% 18/90 [00:01<00:07,  9.50it/s]\u001b[A\n",
            " 21% 19/90 [00:01<00:07,  9.22it/s]\u001b[A\n",
            " 22% 20/90 [00:02<00:07,  9.33it/s]\u001b[A\n",
            " 23% 21/90 [00:02<00:07,  9.03it/s]\u001b[A\n",
            " 24% 22/90 [00:02<00:07,  9.13it/s]\u001b[A\n",
            " 26% 23/90 [00:02<00:07,  9.25it/s]\u001b[A\n",
            " 27% 24/90 [00:02<00:07,  9.16it/s]\u001b[A\n",
            " 28% 25/90 [00:02<00:07,  9.15it/s]\u001b[A\n",
            " 29% 26/90 [00:02<00:06,  9.20it/s]\u001b[A\n",
            " 30% 27/90 [00:02<00:06,  9.32it/s]\u001b[A\n",
            " 31% 28/90 [00:02<00:06,  9.32it/s]\u001b[A\n",
            " 32% 29/90 [00:03<00:06,  9.03it/s]\u001b[A\n",
            " 33% 30/90 [00:03<00:06,  9.23it/s]\u001b[A\n",
            " 34% 31/90 [00:03<00:06,  9.34it/s]\u001b[A\n",
            " 36% 32/90 [00:03<00:06,  9.20it/s]\u001b[A\n",
            " 37% 33/90 [00:03<00:06,  9.37it/s]\u001b[A\n",
            " 38% 34/90 [00:03<00:05,  9.41it/s]\u001b[A\n",
            " 39% 35/90 [00:03<00:05,  9.17it/s]\u001b[A\n",
            " 40% 36/90 [00:03<00:05,  9.11it/s]\u001b[A\n",
            " 41% 37/90 [00:03<00:05,  9.23it/s]\u001b[A\n",
            " 42% 38/90 [00:04<00:05,  9.01it/s]\u001b[A\n",
            " 43% 39/90 [00:04<00:05,  9.17it/s]\u001b[A\n",
            " 44% 40/90 [00:04<00:05,  9.26it/s]\u001b[A\n",
            " 46% 41/90 [00:04<00:05,  9.23it/s]\u001b[A\n",
            " 47% 42/90 [00:04<00:05,  9.29it/s]\u001b[A\n",
            " 48% 43/90 [00:04<00:05,  9.35it/s]\u001b[A\n",
            " 49% 44/90 [00:04<00:04,  9.30it/s]\u001b[A\n",
            " 50% 45/90 [00:04<00:04,  9.25it/s]\u001b[A\n",
            " 51% 46/90 [00:04<00:04,  9.37it/s]\u001b[A\n",
            " 52% 47/90 [00:04<00:04,  9.28it/s]\u001b[A\n",
            " 53% 48/90 [00:05<00:04,  9.10it/s]\u001b[A\n",
            " 54% 49/90 [00:05<00:04,  9.19it/s]\u001b[A\n",
            " 56% 50/90 [00:05<00:04,  9.15it/s]\u001b[A\n",
            " 57% 51/90 [00:05<00:04,  9.29it/s]\u001b[A\n",
            " 58% 52/90 [00:05<00:04,  8.95it/s]\u001b[A\n",
            " 59% 53/90 [00:05<00:04,  8.24it/s]\u001b[A\n",
            " 60% 54/90 [00:05<00:04,  7.91it/s]\u001b[A\n",
            " 61% 55/90 [00:05<00:04,  7.89it/s]\u001b[A\n",
            " 62% 56/90 [00:06<00:04,  7.39it/s]\u001b[A\n",
            " 63% 57/90 [00:06<00:04,  7.47it/s]\u001b[A\n",
            " 64% 58/90 [00:06<00:04,  7.52it/s]\u001b[A\n",
            " 66% 59/90 [00:06<00:04,  7.67it/s]\u001b[A\n",
            " 67% 60/90 [00:06<00:03,  7.84it/s]\u001b[A\n",
            " 68% 61/90 [00:06<00:03,  7.87it/s]\u001b[A\n",
            " 69% 62/90 [00:06<00:03,  7.59it/s]\u001b[A\n",
            " 70% 63/90 [00:07<00:03,  7.37it/s]\u001b[A\n",
            " 71% 64/90 [00:07<00:03,  7.55it/s]\u001b[A\n",
            " 72% 65/90 [00:07<00:03,  7.08it/s]\u001b[A\n",
            " 73% 66/90 [00:07<00:03,  7.11it/s]\u001b[A\n",
            " 74% 67/90 [00:07<00:03,  6.86it/s]\u001b[A\n",
            " 76% 68/90 [00:07<00:02,  7.37it/s]\u001b[A\n",
            " 77% 69/90 [00:07<00:02,  7.91it/s]\u001b[A\n",
            " 78% 70/90 [00:07<00:02,  8.18it/s]\u001b[A\n",
            " 79% 71/90 [00:08<00:02,  8.53it/s]\u001b[A\n",
            " 80% 72/90 [00:08<00:02,  8.78it/s]\u001b[A\n",
            " 81% 73/90 [00:08<00:02,  8.49it/s]\u001b[A\n",
            " 82% 74/90 [00:08<00:01,  8.80it/s]\u001b[A\n",
            " 83% 75/90 [00:08<00:01,  9.01it/s]\u001b[A\n",
            " 84% 76/90 [00:08<00:01,  8.83it/s]\u001b[A\n",
            " 86% 77/90 [00:08<00:01,  9.08it/s]\u001b[A\n",
            " 87% 78/90 [00:08<00:01,  9.16it/s]\u001b[A\n",
            " 88% 79/90 [00:08<00:01,  9.12it/s]\u001b[A\n",
            " 89% 80/90 [00:09<00:01,  9.32it/s]\u001b[A\n",
            " 90% 81/90 [00:09<00:00,  9.25it/s]\u001b[A\n",
            " 91% 82/90 [00:09<00:00,  9.16it/s]\u001b[A\n",
            " 92% 83/90 [00:09<00:00,  8.90it/s]\u001b[A\n",
            " 93% 84/90 [00:09<00:00,  9.04it/s]\u001b[A\n",
            " 94% 85/90 [00:09<00:00,  9.08it/s]\u001b[A\n",
            " 96% 86/90 [00:09<00:00,  9.18it/s]\u001b[A\n",
            " 97% 87/90 [00:09<00:00,  9.21it/s]\u001b[A\n",
            " 98% 88/90 [00:09<00:00,  9.27it/s]\u001b[A\n",
            " 99% 89/90 [00:10<00:00,  9.35it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.02817482128739357, 'eval_runtime': 10.2136, 'eval_samples_per_second': 17.526, 'eval_steps_per_second': 8.812, 'epoch': 1.0}\n",
            " 33% 100/303 [04:41<09:07,  2.70s/it]\n",
            "100% 90/90 [00:10<00:00,  9.29it/s]\u001b[A\n",
            "{'loss': 0.0331, 'grad_norm': 0.8888354301452637, 'learning_rate': 7.272727272727273e-05, 'epoch': 1.19}\n",
            "{'loss': 0.0354, 'grad_norm': 0.19710029661655426, 'learning_rate': 6.482213438735179e-05, 'epoch': 1.39}\n",
            "{'loss': 0.0273, 'grad_norm': 0.3823763430118561, 'learning_rate': 5.6916996047430836e-05, 'epoch': 1.59}\n",
            "{'loss': 0.0212, 'grad_norm': 0.4676215946674347, 'learning_rate': 4.901185770750988e-05, 'epoch': 1.79}\n",
            "{'loss': 0.0152, 'grad_norm': 0.4205286204814911, 'learning_rate': 4.110671936758894e-05, 'epoch': 1.99}\n",
            " 66% 200/303 [09:12<04:44,  2.77s/it]\n",
            "  0% 0/90 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/90 [00:00<00:04, 18.94it/s]\u001b[A\n",
            "  4% 4/90 [00:00<00:07, 11.19it/s]\u001b[A\n",
            "  7% 6/90 [00:00<00:08, 10.23it/s]\u001b[A\n",
            "  9% 8/90 [00:00<00:08,  9.65it/s]\u001b[A\n",
            " 11% 10/90 [00:00<00:08,  9.45it/s]\u001b[A\n",
            " 12% 11/90 [00:01<00:08,  9.39it/s]\u001b[A\n",
            " 13% 12/90 [00:01<00:08,  9.14it/s]\u001b[A\n",
            " 14% 13/90 [00:01<00:08,  9.20it/s]\u001b[A\n",
            " 16% 14/90 [00:01<00:08,  9.28it/s]\u001b[A\n",
            " 17% 15/90 [00:01<00:08,  9.31it/s]\u001b[A\n",
            " 18% 16/90 [00:01<00:08,  9.03it/s]\u001b[A\n",
            " 19% 17/90 [00:01<00:07,  9.13it/s]\u001b[A\n",
            " 20% 18/90 [00:01<00:07,  9.23it/s]\u001b[A\n",
            " 21% 19/90 [00:01<00:07,  8.98it/s]\u001b[A\n",
            " 22% 20/90 [00:02<00:07,  9.14it/s]\u001b[A\n",
            " 23% 21/90 [00:02<00:07,  9.01it/s]\u001b[A\n",
            " 24% 22/90 [00:02<00:07,  9.18it/s]\u001b[A\n",
            " 26% 23/90 [00:02<00:07,  9.28it/s]\u001b[A\n",
            " 27% 24/90 [00:02<00:07,  9.13it/s]\u001b[A\n",
            " 28% 25/90 [00:02<00:07,  9.12it/s]\u001b[A\n",
            " 29% 26/90 [00:02<00:06,  9.21it/s]\u001b[A\n",
            " 30% 27/90 [00:02<00:06,  9.18it/s]\u001b[A\n",
            " 31% 28/90 [00:02<00:06,  9.34it/s]\u001b[A\n",
            " 32% 29/90 [00:03<00:06,  9.37it/s]\u001b[A\n",
            " 33% 30/90 [00:03<00:06,  9.02it/s]\u001b[A\n",
            " 34% 31/90 [00:03<00:06,  8.82it/s]\u001b[A\n",
            " 36% 32/90 [00:03<00:06,  9.05it/s]\u001b[A\n",
            " 37% 33/90 [00:03<00:06,  9.03it/s]\u001b[A\n",
            " 38% 34/90 [00:03<00:06,  8.29it/s]\u001b[A\n",
            " 39% 35/90 [00:03<00:06,  7.97it/s]\u001b[A\n",
            " 40% 36/90 [00:03<00:07,  7.47it/s]\u001b[A\n",
            " 41% 37/90 [00:04<00:07,  7.56it/s]\u001b[A\n",
            " 42% 38/90 [00:04<00:06,  7.71it/s]\u001b[A\n",
            " 43% 39/90 [00:04<00:06,  7.57it/s]\u001b[A\n",
            " 44% 40/90 [00:04<00:06,  7.67it/s]\u001b[A\n",
            " 46% 41/90 [00:04<00:06,  7.79it/s]\u001b[A\n",
            " 47% 42/90 [00:04<00:06,  7.79it/s]\u001b[A\n",
            " 48% 43/90 [00:04<00:06,  7.80it/s]\u001b[A\n",
            " 49% 44/90 [00:05<00:06,  7.48it/s]\u001b[A\n",
            " 50% 45/90 [00:05<00:06,  7.49it/s]\u001b[A\n",
            " 51% 46/90 [00:05<00:06,  7.31it/s]\u001b[A\n",
            " 52% 47/90 [00:05<00:06,  6.84it/s]\u001b[A\n",
            " 53% 48/90 [00:05<00:06,  6.73it/s]\u001b[A\n",
            " 54% 49/90 [00:05<00:05,  7.34it/s]\u001b[A\n",
            " 56% 50/90 [00:05<00:05,  7.82it/s]\u001b[A\n",
            " 57% 51/90 [00:05<00:04,  8.25it/s]\u001b[A\n",
            " 58% 52/90 [00:06<00:04,  8.54it/s]\u001b[A\n",
            " 59% 53/90 [00:06<00:04,  8.75it/s]\u001b[A\n",
            " 60% 54/90 [00:06<00:04,  8.97it/s]\u001b[A\n",
            " 61% 55/90 [00:06<00:03,  9.12it/s]\u001b[A\n",
            " 62% 56/90 [00:06<00:03,  8.57it/s]\u001b[A\n",
            " 63% 57/90 [00:06<00:03,  8.71it/s]\u001b[A\n",
            " 64% 58/90 [00:06<00:03,  8.83it/s]\u001b[A\n",
            " 66% 59/90 [00:06<00:03,  8.97it/s]\u001b[A\n",
            " 67% 60/90 [00:06<00:03,  9.03it/s]\u001b[A\n",
            " 68% 61/90 [00:07<00:03,  9.06it/s]\u001b[A\n",
            " 69% 62/90 [00:07<00:03,  8.94it/s]\u001b[A\n",
            " 70% 63/90 [00:07<00:02,  9.05it/s]\u001b[A\n",
            " 71% 64/90 [00:07<00:02,  9.11it/s]\u001b[A\n",
            " 72% 65/90 [00:07<00:02,  8.89it/s]\u001b[A\n",
            " 73% 66/90 [00:07<00:02,  8.78it/s]\u001b[A\n",
            " 74% 67/90 [00:07<00:02,  8.96it/s]\u001b[A\n",
            " 76% 68/90 [00:07<00:02,  9.03it/s]\u001b[A\n",
            " 77% 69/90 [00:07<00:02,  9.08it/s]\u001b[A\n",
            " 78% 70/90 [00:08<00:02,  9.13it/s]\u001b[A\n",
            " 79% 71/90 [00:08<00:02,  9.29it/s]\u001b[A\n",
            " 80% 72/90 [00:08<00:01,  9.29it/s]\u001b[A\n",
            " 81% 73/90 [00:08<00:01,  9.11it/s]\u001b[A\n",
            " 82% 74/90 [00:08<00:01,  9.17it/s]\u001b[A\n",
            " 83% 75/90 [00:08<00:01,  8.75it/s]\u001b[A\n",
            " 84% 76/90 [00:08<00:01,  8.92it/s]\u001b[A\n",
            " 86% 77/90 [00:08<00:01,  9.05it/s]\u001b[A\n",
            " 87% 78/90 [00:08<00:01,  8.99it/s]\u001b[A\n",
            " 88% 79/90 [00:09<00:01,  9.15it/s]\u001b[A\n",
            " 89% 80/90 [00:09<00:01,  9.19it/s]\u001b[A\n",
            " 90% 81/90 [00:09<00:00,  9.17it/s]\u001b[A\n",
            " 91% 82/90 [00:09<00:00,  9.29it/s]\u001b[A\n",
            " 92% 83/90 [00:09<00:00,  9.15it/s]\u001b[A\n",
            " 93% 84/90 [00:09<00:00,  8.75it/s]\u001b[A\n",
            " 94% 85/90 [00:09<00:00,  8.94it/s]\u001b[A\n",
            " 96% 86/90 [00:09<00:00,  9.07it/s]\u001b[A\n",
            " 97% 87/90 [00:09<00:00,  9.10it/s]\u001b[A\n",
            " 98% 88/90 [00:10<00:00,  9.19it/s]\u001b[A\n",
            " 99% 89/90 [00:10<00:00,  9.22it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.010438794270157814, 'eval_runtime': 10.3228, 'eval_samples_per_second': 17.34, 'eval_steps_per_second': 8.719, 'epoch': 1.99}\n",
            " 66% 200/303 [09:22<04:44,  2.77s/it]\n",
            "100% 90/90 [00:10<00:00,  9.33it/s]\u001b[A\n",
            "{'loss': 0.0098, 'grad_norm': 0.31712862849235535, 'learning_rate': 3.320158102766799e-05, 'epoch': 2.18}\n",
            "{'loss': 0.0102, 'grad_norm': 0.3866107761859894, 'learning_rate': 2.5296442687747035e-05, 'epoch': 2.38}\n",
            "{'loss': 0.0044, 'grad_norm': 0.35511112213134766, 'learning_rate': 1.739130434782609e-05, 'epoch': 2.58}\n",
            "{'loss': 0.0026, 'grad_norm': 0.7723092436790466, 'learning_rate': 9.486166007905138e-06, 'epoch': 2.78}\n",
            "{'loss': 0.0011, 'grad_norm': 0.3388473391532898, 'learning_rate': 1.5810276679841897e-06, 'epoch': 2.98}\n",
            " 99% 300/303 [13:53<00:08,  2.80s/it]\n",
            "  0% 0/90 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/90 [00:00<00:04, 18.63it/s]\u001b[A\n",
            "  4% 4/90 [00:00<00:07, 11.52it/s]\u001b[A\n",
            "  7% 6/90 [00:00<00:08, 10.34it/s]\u001b[A\n",
            "  9% 8/90 [00:00<00:08,  9.73it/s]\u001b[A\n",
            " 11% 10/90 [00:00<00:08,  9.57it/s]\u001b[A\n",
            " 12% 11/90 [00:01<00:08,  9.51it/s]\u001b[A\n",
            " 13% 12/90 [00:01<00:08,  9.54it/s]\u001b[A\n",
            " 14% 13/90 [00:01<00:08,  9.57it/s]\u001b[A\n",
            " 16% 14/90 [00:01<00:07,  9.56it/s]\u001b[A\n",
            " 17% 15/90 [00:01<00:07,  9.51it/s]\u001b[A\n",
            " 18% 16/90 [00:01<00:07,  9.37it/s]\u001b[A\n",
            " 19% 17/90 [00:01<00:07,  9.15it/s]\u001b[A\n",
            " 20% 18/90 [00:01<00:07,  9.34it/s]\u001b[A\n",
            " 21% 19/90 [00:01<00:07,  9.11it/s]\u001b[A\n",
            " 22% 20/90 [00:02<00:07,  9.31it/s]\u001b[A\n",
            " 23% 21/90 [00:02<00:07,  9.46it/s]\u001b[A\n",
            " 24% 22/90 [00:02<00:07,  9.31it/s]\u001b[A\n",
            " 26% 23/90 [00:02<00:07,  9.34it/s]\u001b[A\n",
            " 27% 24/90 [00:02<00:07,  9.31it/s]\u001b[A\n",
            " 28% 25/90 [00:02<00:07,  9.12it/s]\u001b[A\n",
            " 29% 26/90 [00:02<00:07,  8.80it/s]\u001b[A\n",
            " 30% 27/90 [00:02<00:06,  9.10it/s]\u001b[A\n",
            " 31% 28/90 [00:02<00:06,  9.08it/s]\u001b[A\n",
            " 32% 29/90 [00:03<00:06,  9.20it/s]\u001b[A\n",
            " 33% 30/90 [00:03<00:06,  9.17it/s]\u001b[A\n",
            " 34% 31/90 [00:03<00:06,  9.18it/s]\u001b[A\n",
            " 36% 32/90 [00:03<00:06,  9.25it/s]\u001b[A\n",
            " 37% 33/90 [00:03<00:06,  9.36it/s]\u001b[A\n",
            " 38% 34/90 [00:03<00:06,  9.30it/s]\u001b[A\n",
            " 39% 35/90 [00:03<00:06,  9.09it/s]\u001b[A\n",
            " 40% 36/90 [00:03<00:06,  8.85it/s]\u001b[A\n",
            " 41% 37/90 [00:03<00:05,  8.98it/s]\u001b[A\n",
            " 42% 38/90 [00:04<00:05,  9.00it/s]\u001b[A\n",
            " 43% 39/90 [00:04<00:05,  9.06it/s]\u001b[A\n",
            " 44% 40/90 [00:04<00:05,  8.98it/s]\u001b[A\n",
            " 46% 41/90 [00:04<00:05,  9.23it/s]\u001b[A\n",
            " 47% 42/90 [00:04<00:05,  9.26it/s]\u001b[A\n",
            " 48% 43/90 [00:04<00:05,  9.29it/s]\u001b[A\n",
            " 49% 44/90 [00:04<00:04,  9.36it/s]\u001b[A\n",
            " 50% 45/90 [00:04<00:04,  9.06it/s]\u001b[A\n",
            " 51% 46/90 [00:04<00:04,  9.24it/s]\u001b[A\n",
            " 52% 47/90 [00:04<00:04,  9.42it/s]\u001b[A\n",
            " 53% 48/90 [00:05<00:04,  9.20it/s]\u001b[A\n",
            " 54% 49/90 [00:05<00:04,  9.21it/s]\u001b[A\n",
            " 56% 50/90 [00:05<00:04,  9.19it/s]\u001b[A\n",
            " 57% 51/90 [00:05<00:04,  9.39it/s]\u001b[A\n",
            " 58% 52/90 [00:05<00:04,  9.39it/s]\u001b[A\n",
            " 59% 53/90 [00:05<00:03,  9.31it/s]\u001b[A\n",
            " 60% 54/90 [00:05<00:03,  9.40it/s]\u001b[A\n",
            " 61% 55/90 [00:05<00:03,  8.98it/s]\u001b[A\n",
            " 62% 56/90 [00:05<00:03,  9.03it/s]\u001b[A\n",
            " 63% 57/90 [00:06<00:03,  9.19it/s]\u001b[A\n",
            " 64% 58/90 [00:06<00:03,  9.05it/s]\u001b[A\n",
            " 66% 59/90 [00:06<00:03,  9.17it/s]\u001b[A\n",
            " 67% 60/90 [00:06<00:03,  9.25it/s]\u001b[A\n",
            " 68% 61/90 [00:06<00:03,  9.18it/s]\u001b[A\n",
            " 69% 62/90 [00:06<00:03,  8.93it/s]\u001b[A\n",
            " 70% 63/90 [00:06<00:02,  9.19it/s]\u001b[A\n",
            " 71% 64/90 [00:06<00:02,  8.69it/s]\u001b[A\n",
            " 72% 65/90 [00:06<00:02,  9.04it/s]\u001b[A\n",
            " 73% 66/90 [00:07<00:02,  9.17it/s]\u001b[A\n",
            " 74% 67/90 [00:07<00:02,  9.17it/s]\u001b[A\n",
            " 76% 68/90 [00:07<00:02,  9.33it/s]\u001b[A\n",
            " 77% 69/90 [00:07<00:02,  9.38it/s]\u001b[A\n",
            " 78% 70/90 [00:07<00:02,  9.37it/s]\u001b[A\n",
            " 79% 71/90 [00:07<00:02,  9.41it/s]\u001b[A\n",
            " 80% 72/90 [00:07<00:01,  9.49it/s]\u001b[A\n",
            " 81% 73/90 [00:07<00:01,  9.17it/s]\u001b[A\n",
            " 82% 74/90 [00:07<00:01,  9.03it/s]\u001b[A\n",
            " 83% 75/90 [00:08<00:01,  9.20it/s]\u001b[A\n",
            " 84% 76/90 [00:08<00:01,  9.05it/s]\u001b[A\n",
            " 86% 77/90 [00:08<00:01,  9.18it/s]\u001b[A\n",
            " 87% 78/90 [00:08<00:01,  9.22it/s]\u001b[A\n",
            " 88% 79/90 [00:08<00:01,  9.22it/s]\u001b[A\n",
            " 89% 80/90 [00:08<00:01,  9.25it/s]\u001b[A\n",
            " 90% 81/90 [00:08<00:00,  9.29it/s]\u001b[A\n",
            " 91% 82/90 [00:08<00:00,  9.21it/s]\u001b[A\n",
            " 92% 83/90 [00:08<00:00,  8.68it/s]\u001b[A\n",
            " 93% 84/90 [00:09<00:00,  8.99it/s]\u001b[A\n",
            " 94% 85/90 [00:09<00:00,  9.18it/s]\u001b[A\n",
            " 96% 86/90 [00:09<00:00,  9.06it/s]\u001b[A\n",
            " 97% 87/90 [00:09<00:00,  9.26it/s]\u001b[A\n",
            " 98% 88/90 [00:09<00:00,  9.34it/s]\u001b[A\n",
            " 99% 89/90 [00:09<00:00,  9.18it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.00231983233243227, 'eval_runtime': 9.8056, 'eval_samples_per_second': 18.255, 'eval_steps_per_second': 9.178, 'epoch': 2.98}\n",
            " 99% 300/303 [14:03<00:08,  2.80s/it]\n",
            "100% 90/90 [00:09<00:00,  8.78it/s]\u001b[A\n",
            "{'train_runtime': 851.514, 'train_samples_per_second': 5.662, 'train_steps_per_second': 0.356, 'train_loss': 0.03305250394585306, 'epoch': 3.0}\n",
            "100% 303/303 [14:11<00:00,  2.81s/it]\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Caching is incompatible with gradient checkpointing in LlamaDecoderLayer. Setting `past_key_values=None`.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "{\n",
            "  \"adapter_dir\": \"/content/Hypertension-AI/artifacts/llm/lora_adapter\",\n",
            "  \"validation_metrics\": {\n",
            "    \"num_samples\": 179,\n",
            "    \"hypertension_accuracy\": 0.0,\n",
            "    \"stress_accuracy\": 0.0,\n",
            "    \"joint_exact_match\": 0.0\n",
            "  }\n",
            "}\n",
            "Generating test split: 199 examples [00:00, 77991.64 examples/s]\n",
            "{\n",
            "  \"test_metrics\": {\n",
            "    \"num_samples\": 199,\n",
            "    \"hypertension_accuracy\": 0.0,\n",
            "    \"stress_accuracy\": 0.0,\n",
            "    \"joint_exact_match\": 0.0\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!python /content/Hypertension-AI/src/fine_tune_hypertension_llm_2.py \\\n",
        "  --train-file /content/Hypertension-AI/data/llm/train.jsonl \\\n",
        "  --valid-file /content/Hypertension-AI/data/llm/valid.jsonl \\\n",
        "  --test-file /content/Hypertension-AI/data/llm/test.jsonl \\\n",
        "  --output-dir /content/Hypertension-AI/artifacts/llm \\\n",
        "  --base-model meta-llama/Llama-3.2-1B-Instruct \\\n",
        "  --num-epochs 3 \\\n",
        "  --batch-size 2 \\\n",
        "  --gradient-accumulation 8 \\\n",
        "  --learning-rate 1e-4 \\\n",
        "  --max-length 512 \\\n",
        "  --max-new-tokens 24 \\\n",
        "  --load-in-4bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MTAAUrJshUn0",
        "outputId": "f90df05b-3349-4e3b-fcb8-a7115b0246a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-08 07:19:54.165555: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1759907994.185738   12326 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1759907994.191987   12326 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1759907994.208469   12326 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759907994.208504   12326 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759907994.208508   12326 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759907994.208511   12326 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-08 07:19:54.213321: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "{\n",
            "  \"adapter_dir\": \"/content/Hypertension-AI/artifacts/llm/lora_adapter\",\n",
            "  \"test_metrics\": {\n",
            "    \"num_samples\": 199,\n",
            "    \"hypertension_accuracy\": 0.8341708542713567,\n",
            "    \"stress_accuracy\": 1.0,\n",
            "    \"joint_exact_match\": 0.8341708542713567\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!python /content/Hypertension-AI/src/fine_tune_hypertension_llm_2.py \\\n",
        "  --eval-only \\\n",
        "  --test-file /content/Hypertension-AI/data/llm/test.jsonl \\\n",
        "  --base-model meta-llama/Llama-3.2-1B-Instruct \\\n",
        "  --adapter-dir /content/Hypertension-AI/artifacts/llm/lora_adapter \\\n",
        "  --max-new-tokens 24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LzcBNw5_TLMY",
        "outputId": "2f1024d0-7acc-41ee-91eb-1192303d7b04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-08 07:22:02.779724: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1759908122.799897   12879 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1759908122.805829   12879 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1759908122.820978   12879 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759908122.821005   12879 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759908122.821009   12879 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759908122.821014   12879 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-08 07:22:02.825882: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Generating data split: 199 examples [00:00, 51528.98 examples/s]\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "{\n",
            "  \"n_samples\": 199,\n",
            "  \"hypertension\": {\n",
            "    \"tp\": 70,\n",
            "    \"tn\": 96,\n",
            "    \"fp\": 0,\n",
            "    \"fn\": 33,\n",
            "    \"accuracy\": 0.8341708542713567,\n",
            "    \"precision\": 1.0,\n",
            "    \"recall\": 0.6796116504854369,\n",
            "    \"specificity\": 1.0,\n",
            "    \"f1\": 0.8092485549132947\n",
            "  },\n",
            "  \"stress\": {\n",
            "    \"accuracy\": 1.0\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!python /content/Hypertension-AI/src/evaluate_llm_detailed.py \\\n",
        "  --base-model meta-llama/Llama-3.2-1B-Instruct \\\n",
        "  --adapter-dir /content/Hypertension-AI/artifacts/llm/lora_adapter \\\n",
        "  --data-file /content/Hypertension-AI/data/llm/test.jsonl \\\n",
        "  --max-new-tokens 24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6QZjxlYOxdV"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Using 8B Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SbNo9C4IOvGJ",
        "outputId": "a0f7cf29-c454-48a6-968c-dcd79bb73fe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokenizer_config.json: 100% 55.4k/55.4k [00:00<00:00, 88.3MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 12.9MB/s]\n",
            "special_tokens_map.json: 100% 296/296 [00:00<00:00, 2.75MB/s]\n",
            "{\n",
            "  \"train\": 1607,\n",
            "  \"valid\": 179,\n",
            "  \"test\": 199,\n",
            "  \"stress_thresholds\": {\n",
            "    \"low_max\": 6.0,\n",
            "    \"moderate_max\": 10.0\n",
            "  },\n",
            "  \"prompt_style\": \"chat_template_llama\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!python /content/Hypertension-AI/src/prepare_hypertension_llm_data_2.py \\\n",
        "  --data-path /content/Hypertension-AI/data/hypertension_dataset.csv \\\n",
        "  --output-dir /content/Hypertension-AI/data/llm \\\n",
        "  --base-model meta-llama/Llama-3.1-8B-Instruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Vr9CUE1xOvGK",
        "outputId": "ed46cfaa-1f4b-4f64-c634-8a0c6fe25981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-10 18:37:44.000316: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760121464.021449    7429 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760121464.027374    7429 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760121464.042483    7429 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760121464.042506    7429 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760121464.042510    7429 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760121464.042513    7429 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-10 18:37:44.046985: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 4/4 [01:10<00:00, 17.59s/it]\n",
            "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n",
            "  0% 0/303 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "{'loss': 0.1418, 'grad_norm': 0.351162314414978, 'learning_rate': 3.8e-05, 'epoch': 0.2}\n",
            "{'loss': 0.0483, 'grad_norm': 0.2877887785434723, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.4}\n",
            "{'loss': 0.0504, 'grad_norm': 0.26249268651008606, 'learning_rate': 9.644268774703557e-05, 'epoch': 0.6}\n",
            " 23% 70/303 [1:24:36<4:41:28, 72.48s/it]"
          ]
        }
      ],
      "source": [
        "!python /content/Hypertension-AI/src/fine_tune_hypertension_llm_2.py \\\n",
        "  --train-file /content/Hypertension-AI/data/llm/train.jsonl \\\n",
        "  --valid-file /content/Hypertension-AI/data/llm/valid.jsonl \\\n",
        "  --test-file /content/Hypertension-AI/data/llm/test.jsonl \\\n",
        "  --output-dir /content/Hypertension-AI/artifacts/llm \\\n",
        "  --base-model meta-llama/Llama-3.1-8B-Instruct \\\n",
        "  --num-epochs 3 \\\n",
        "  --batch-size 2 \\\n",
        "  --gradient-accumulation 8 \\\n",
        "  --learning-rate 1e-4 \\\n",
        "  --max-length 512 \\\n",
        "  --max-new-tokens 24 \\\n",
        "  --load-in-4bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "U1eAtSeuOvGK"
      },
      "outputs": [],
      "source": [
        "!python /content/Hypertension-AI/src/fine_tune_hypertension_llm_2.py \\\n",
        "  --eval-only \\\n",
        "  --test-file /content/Hypertension-AI/data/llm/test.jsonl \\\n",
        "  --base-model meta-llama/Llama-3.1-8B-Instruct \\\n",
        "  --adapter-dir /content/Hypertension-AI/artifacts/llm/lora_adapter \\\n",
        "  --max-new-tokens 24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kOeUvdQROvGK"
      },
      "outputs": [],
      "source": [
        "!python /content/Hypertension-AI/src/evaluate_llm_detailed.py \\\n",
        "  --base-model meta-llama/Llama-3.1-8B-Instruct \\\n",
        "  --adapter-dir /content/Hypertension-AI/artifacts/llm/lora_adapter \\\n",
        "  --data-file /content/Hypertension-AI/data/llm/test.jsonl \\\n",
        "  --max-new-tokens 24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad8y4Pj0id7M"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Using tensorblock/medllama3-v20-GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "250fdd2e-cea0-4b73-ac3a-c8ce54a12ca4",
        "id": "erruVExxid7M"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Hypertension-AI/src/prepare_hypertension_llm_data_2.py\", line 128, in <module>\n",
            "    main()\n",
            "  File \"/content/Hypertension-AI/src/prepare_hypertension_llm_data_2.py\", line 99, in main\n",
            "    tokenizer = AutoTokenizer.from_pretrained(args.base_model, use_fast=True)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\", line 1159, in from_pretrained\n",
            "    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\", line 2097, in from_pretrained\n",
            "    return cls._from_pretrained(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\", line 2135, in _from_pretrained\n",
            "    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\", line 2343, in _from_pretrained\n",
            "    tokenizer = cls(*init_inputs, **init_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/llama/tokenization_llama.py\", line 171, in __init__\n",
            "    self.sp_model = self.get_spm_processor(kwargs.pop(\"from_slow\", False))\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/llama/tokenization_llama.py\", line 198, in get_spm_processor\n",
            "    tokenizer.Load(self.vocab_file)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sentencepiece/__init__.py\", line 961, in Load\n",
            "    return self.LoadFromFile(model_file)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sentencepiece/__init__.py\", line 316, in LoadFromFile\n",
            "    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: not a string\n"
          ]
        }
      ],
      "source": [
        "!python /content/Hypertension-AI/src/prepare_hypertension_llm_data_2.py \\\n",
        "  --data-path /content/Hypertension-AI/data/hypertension_dataset.csv \\\n",
        "  --output-dir /content/Hypertension-AI/data/llm \\\n",
        "  --base-model tensorblock/medllama3-v20-GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "ed46cfaa-1f4b-4f64-c634-8a0c6fe25981",
        "id": "aKTUHIZiid7M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-10 18:37:44.000316: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760121464.021449    7429 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760121464.027374    7429 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760121464.042483    7429 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760121464.042506    7429 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760121464.042510    7429 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760121464.042513    7429 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-10 18:37:44.046985: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 4/4 [01:10<00:00, 17.59s/it]\n",
            "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n",
            "  0% 0/303 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "{'loss': 0.1418, 'grad_norm': 0.351162314414978, 'learning_rate': 3.8e-05, 'epoch': 0.2}\n",
            "{'loss': 0.0483, 'grad_norm': 0.2877887785434723, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.4}\n",
            "{'loss': 0.0504, 'grad_norm': 0.26249268651008606, 'learning_rate': 9.644268774703557e-05, 'epoch': 0.6}\n",
            " 23% 70/303 [1:24:36<4:41:28, 72.48s/it]"
          ]
        }
      ],
      "source": [
        "!python /content/Hypertension-AI/src/fine_tune_hypertension_llm_2.py \\\n",
        "  --train-file /content/Hypertension-AI/data/llm/train.jsonl \\\n",
        "  --valid-file /content/Hypertension-AI/data/llm/valid.jsonl \\\n",
        "  --test-file /content/Hypertension-AI/data/llm/test.jsonl \\\n",
        "  --output-dir /content/Hypertension-AI/artifacts/llm \\\n",
        "  --base-model tensorblock/medllama3-v20-GGUF \\\n",
        "  --num-epochs 3 \\\n",
        "  --batch-size 2 \\\n",
        "  --gradient-accumulation 8 \\\n",
        "  --learning-rate 1e-4 \\\n",
        "  --max-length 512 \\\n",
        "  --max-new-tokens 24 \\\n",
        "  --load-in-4bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4iaO2RbDid7N"
      },
      "outputs": [],
      "source": [
        "!python /content/Hypertension-AI/src/fine_tune_hypertension_llm_2.py \\\n",
        "  --eval-only \\\n",
        "  --test-file /content/Hypertension-AI/data/llm/test.jsonl \\\n",
        "  --base-model tensorblock/medllama3-v20-GGUF \\\n",
        "  --adapter-dir /content/Hypertension-AI/artifacts/llm/lora_adapter \\\n",
        "  --max-new-tokens 24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ibRp7Oqjid7N"
      },
      "outputs": [],
      "source": [
        "!python /content/Hypertension-AI/src/evaluate_llm_detailed.py \\\n",
        "  --base-model tensorblock/medllama3-v20-GGUF \\\n",
        "  --adapter-dir /content/Hypertension-AI/artifacts/llm/lora_adapter \\\n",
        "  --data-file /content/Hypertension-AI/data/llm/test.jsonl \\\n",
        "  --max-new-tokens 24"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}